{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting a Logistic Regression parameter\n",
    "You are now going to practice extracting an important parameter of the logistic regression model. The logistic regression has a few other parameters you will not explore here but you can review them in the scikit-learn.org documentation for the LogisticRegression() module under 'Attributes'.  \n",
    "\n",
    "This parameter is important for understanding the direction and magnitude of the effect the variables have on the target.  \n",
    "\n",
    "In this exercise we will extract the coefficient parameter (found in the coef_ attribute), zip it up with the original column names, and see which variables had the largest positive effect on the target variable.  \n",
    "\n",
    "You will have available:  \n",
    "\n",
    "A logistic regression model object named log_reg_clf  \n",
    "The X_train DataFrame  \n",
    "sklearn and pandas have been imported for you.  \n",
    "\n",
    "### Instructions\n",
    "Create a list of the original column names used in the training DataFrame.  \n",
    "Extract the coefficients of the logistic regression estimator.  \n",
    "Create a DataFrame of coefficients and variable names & view it. \n",
    "Print out the top 3 'positive' variables based on the coefficient size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = list(X_train.columns)\n",
    "\n",
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]\n",
    "\n",
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame({'Variable' : original_variables, 'Coefficient': model_coefficients})\n",
    "print(coefficient_df)\n",
    "\n",
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by='Coefficient', axis=0, ascending=False)[0:3]\n",
    "print(top_three_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a Random Forest parameter\n",
    "You will now translate the work previously undertaken on the logistic regression model to a random forest model.   A parameter of this model is, for a given tree, how it decided to split at each level.  \n",
    "\n",
    "This analysis is not as useful as the coefficients of logistic regression as you will be unlikely to ever explore every split and every tree in a random forest model. However, it is a very useful exercise to peak under the hood at what the model is doing.   \n",
    "\n",
    "In this exercise we will extract a single tree from our random forest model, visualize it and programmatically extract one of the splits.   \n",
    "\n",
    "You have available:  \n",
    "\n",
    "A random forest model object, rf_clf  \n",
    "An image of the top of the chosen decision tree, tree_viz_image   \n",
    "The X_train DataFrame & the original_variables list  \n",
    "### Instructions\n",
    "Extract the 7th tree (6th index) from the random forest model.  \n",
    "Visualize this tree (tree_viz_image) to see the split decisions.  \n",
    "Extract the feature & level of the top split.  \n",
    "Print out the feature and level together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# Visualize the graph using the provided image\n",
    "imgplot = plt.imshow(tree_viz_image)\n",
    "plt.show()\n",
    "\n",
    "# Extract the parameters and level of the top (index 0) node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Random Forest Hyperparameters\n",
    "Understanding what hyperparameters are available and the impact of different hyperparameters is a core skill for any data scientist. As models become more complex, there are many different settings you can set, but only some will have a large impact on your model.   \n",
    "\n",
    "You will now assess an existing random forest model (it has some bad choices for hyperparameters!) and then make better choices for a new random forest model and assess its performance.  \n",
    "\n",
    "You will have available:  \n",
    "\n",
    "X_train, X_test, y_train, y_test DataFrames  \n",
    "An existing pre-trained random forest estimator, rf_clf_old  \n",
    "The predictions of the existing random forest estimator on the test set, rf_old_predictions  \n",
    "### Instructions \n",
    "Print out the hyperparameters of the existing random forest classifier by printing the estimator and then create a confusion matrix and accuracy score from it.    \n",
    "The test set y_test and the old predictions rf_old_predictions will be quite useful!   \n",
    "Assess the performance of the new random forest classifier. Create the confusion matrix and accuracy score and print them out. How does this compare to the first model you were given?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the old estimator, notice which hyperparameter is badly set\n",
    "print(rf_clf_old)\n",
    "\n",
    "# Get confusion matrix & accuracy for the old rf_model\n",
    "print(\"Confusion Matrix: \\n\\n {} \\n Accuracy Score: \\n\\n {}\".format(\n",
    "  confusion_matrix(y_test, rf_old_predictions),\n",
    "  accuracy_score(y_test, rf_old_predictions))) \n",
    "\n",
    "# Create a new random forest classifier with better hyperparamaters\n",
    "rf_clf_new = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# Fit this to the data and obtain predictions\n",
    "rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Assess the new model (using new predictions!)\n",
    "print(\"Confusion Matrix: \\n\\n\", confusion_matrix(y_test, rf_new_predictions))\n",
    "print(\"Accuracy Score: \\n\\n\", accuracy_score(y_test, rf_new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of KNN\n",
    "To apply the concepts learned in the prior exercise, it is good practice to try out learnings on a new algorithm.   The k-nearest-neighbors algorithm is not as popular as it used to be but can still be an excellent choice for data that has groups of data that behave similarly. Could this be the case for our credit card users?  \n",
    "\n",
    "In this case you will try out several different values for one of the core hyperparameters for the knn algorithm and compare performance.  \n",
    "\n",
    "You will have available:  \n",
    "\n",
    "X_train, X_test, y_train, y_test DataFrames  \n",
    "### Instructions\n",
    "Build a knn estimator for the following values of n_neighbors [5,10,20].  \n",
    "Fit each to the training data and produce predictions.  \n",
    "Get an accuracy score for each model and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a knn estimator for each value of n_neighbours\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Fit each to the training data & produce predictions\n",
    "knn_5_predictions = knn_5.fit(X_train, y_train).predict(X_test)\n",
    "knn_10_predictions = knn_10.fit(X_train, y_train).predict(X_test)\n",
    "knn_20_predictions = knn_20.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Get an accuracy score for each of the models\n",
    "knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)\n",
    "knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)\n",
    "knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)\n",
    "print(\"The accuracy of 5, 10, 20 neighbours was {}, {}, {}\".format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating Hyperparameter Choice\n",
    "Finding the best hyperparameter of interest without writing hundreds of lines of code for hundreds of models is an important efficiency gain that will greatly assist your future machine learning model building.  \n",
    "\n",
    "An important hyperparameter for the GBM algorithm is the learning rate. But which learning rate is best for this problem? By writing a loop to search through a number of possibilities, collating these and viewing them you can find the best one.   \n",
    "\n",
    "Possible learning rates to try include 0.001, 0.01, 0.05, 0.1, 0.2 and 0.5   \n",
    "\n",
    "You will have available X_train, X_test, y_train & y_test datasets, and GradientBoostingClassifier has been imported for you.    \n",
    "\n",
    "### Instructions\n",
    "Create a learning_rates list for the learning rates, and a results_list to hold the accuracy score of your predictions.  \n",
    "Write a loop to create a GBM model for each learning rate mentioned and create predictions for each model.  \n",
    "Save the learning rate and accuracy score to a results_list.  \n",
    "Turn the results list into a DataFrame and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rates & results storage\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results_list = []\n",
    "\n",
    "# Create the for loop to evaluate model predictions for each learning rate\n",
    "for lr in learning_rates:\n",
    "    model = GradientBoostingClassifier(learning_rate=lr)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    # Save the learning rate and accuracy score\n",
    "    results_list.append([lr, accuracy_score(y_test, predictions)])\n",
    "\n",
    "# Gather everything into a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Learning Curves\n",
    "If we want to test many different values for a single hyperparameter it can be difficult to easily view that in the form of a DataFrame. Previously you learned about a nice trick to analyze this. A graph called a 'learning curve' can nicely demonstrate the effect of increasing or decreasing a particular hyperparameter on the final result.   \n",
    "\n",
    "Instead of testing only a few values for the learning rate, you will test many to easily see the effect of this hyperparameter across a large range of values. A useful function from NumPy is np.linspace(start, end, num) which allows you to create a number of values (num) evenly spread within an interval (start, end) that you specify.  \n",
    "\n",
    "You will have available X_train, X_test, y_train & y_test datasets.\n",
    "\n",
    "### Instructions\n",
    "Create a list of 30 learning rates evenly spread between 0.01 and 2.  \n",
    "Create a similar loop to last exercise but just save out accuracy scores to a list.  \n",
    "Plot the learning rates against the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01, 2.00, num=30)\n",
    "accuracies = []\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "  \t# Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
