{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many repetitions of sounds\n",
    "In this exercise, you'll start with perhaps the simplest classification technique: averaging across dimensions of a dataset and visually inspecting the result.   \n",
    "You'll use the heartbeat data described in the last chapter. Some recordings are normal heartbeat activity, while others are abnormal activity. Let's see if you can spot the difference.   \n",
    "Two DataFrames, normal and abnormal, each with the shape of (n_times_points, n_audio_files) containing the audio for several heartbeats are available in your workspace. Also, the sampling frequency is loaded into a variable called sfreq. A convenience plotting function show_plot_and_make_titles() is also available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "First, create the time array for these audio files (all audios are the same length).      \n",
    "Then, stack the values of the two DataFrames together (normal and abnormal, in that order) so that you have a single array of shape (n_audio_files, n_times_points).     \n",
    "Finally, use the code provided to loop through each list item / axis, and plot the audio over time in the corresponding axis object.    \n",
    "You'll plot normal heartbeats in the left column, and abnormal ones in the right column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
    "\n",
    "# Calculate the time array\n",
    "time = np.arange(len(normal)) / sfreq\n",
    "\n",
    "# Stack the normal/abnormal audio so you can loop and plot\n",
    "stacked_audio = np.hstack([normal, abnormal]).T\n",
    "\n",
    "# Loop through each audio file / ax object and plot\n",
    "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
    "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
    "    ax.plot(time, iaudio)\n",
    "show_plot_and_make_titles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invariance in time\n",
    "While you should always start by visualizing your raw data, this is often uninformative when it comes to discriminating between two classes of data points. Data is usually noisy or exhibits complex patterns that aren't discoverable by the naked eye.   \n",
    "Another common technique to find simple differences between two sets of data is to average across multiple instances of the same class. This may remove noise and reveal underlying patterns (or, it may not).   \n",
    "In this exercise, you'll average across many instances of each class of heartbeat sound.  \n",
    "The two DataFrames (normal and abnormal) and the time array (time) from the previous exercise are available in your workspace.   \n",
    "\n",
    "### Instructions\n",
    "Average across the audio files contained in normal and abnormal, leaving the time dimension.    \n",
    "Visualize these averages over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across the audio files of each DataFrame\n",
    "mean_normal = np.mean(normal, axis=1)\n",
    "mean_abnormal = np.mean(abnormal, axis=1)\n",
    "\n",
    "# Plot each average over time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "ax1.plot(mean_normal.index, mean_normal)\n",
    "ax1.set(title=\"Normal Data\")\n",
    "ax2.plot(mean_abnormal.index, mean_abnormal)\n",
    "ax2.set(title=\"Abnormal Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a classification model\n",
    "While eye-balling differences is a useful way to gain an intuition for the data, let's see if you can operationalize things with a model. In this exercise, you will use each repetition as a datapoint, and each moment in time as a feature to fit a classifier that attempts to predict abnormal vs. normal heartbeats using only the raw data.   \n",
    "We've split the two DataFrames (normal and abnormal) into X_train, X_test, y_train, and y_test.\n",
    "\n",
    "### Instructions\n",
    "Create an instance of the Linear SVC model and fit the model using the training data.    \n",
    "Use the testing data to generate predictions with the model.    \n",
    "Score the model using the provided code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and score them manually\n",
    "predictions = model.predict(X_test)\n",
    "print(sum(predictions == y_test.squeeze()) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the envelope of sound\n",
    "One of the ways you can improve the features available to your model is to remove some of the noise present in the data. In audio data, a common way to do this is to smooth the data and then rectify it so that the total amount of sound energy over time is more distinguishable. You'll do this in the current exercise.\n",
    "\n",
    "A heartbeat file is available in the variable audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw data first\n",
    "audio.plot(figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectify the audio signal\n",
    "audio_rectified = audio.apply(lambda cell: np.abs(cell))\n",
    "\n",
    "# Plot the result\n",
    "audio_rectified.plot(figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth by applying a rolling mean\n",
    "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
    "\n",
    "# Plot the result\n",
    "audio_rectified_smooth.plot(figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating features from the envelope\n",
    "Now that you've removed some of the noisier fluctuations in the audio, let's see if this improves your ability to classify.    \n",
    "audio_rectified_smooth from the previous exercise is available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "Calculate the mean, standard deviation, and maximum value for each heartbeat sound.   \n",
    "Column stack these stats in the same order.   \n",
    "Use cross-validation to fit a model on each CV iteration.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stats\n",
    "means = np.mean(audio_rectified_smooth, axis=0)\n",
    "stds = np.std(audio_rectified_smooth, axis=0)\n",
    "maxs = np.max(audio_rectified_smooth, axis=0)\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative features: The tempogram\n",
    "One benefit of cleaning up your data is that it lets you compute more sophisticated features. For example, the envelope calculation you performed is a common technique in computing tempo and rhythm features.   \n",
    "In this exercise, you'll use librosa to compute some tempo and rhythm features for heartbeat data, and fit a model once more.   \n",
    "Note that librosa functions tend to only operate on numpy arrays instead of DataFrames, so we'll access our Pandas data as a Numpy array with the .values attribute.\n",
    "\n",
    "### Instructions\n",
    "Use librosa to calculate a tempogram of each heartbeat audio.    \n",
    "Calculate the mean, standard deviation, and maximum of each tempogram (this time using DataFrame methods). \n",
    "Column stack these tempo features (mean, standard deviation, and maximum) in the same order.   \n",
    "Score the classifier with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tempo of the sounds\n",
    "tempos = []\n",
    "for col, i_audio in audio.items():\n",
    "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
    "\n",
    "# Convert the list to an array so you can manipulate it more easily\n",
    "tempos = np.array(tempos)\n",
    "\n",
    "# Calculate statistics of each tempo\n",
    "tempos_mean = tempos.mean(axis=-1)\n",
    "tempos_std = tempos.std(axis=-1)\n",
    "tempos_max = tempos.max(axis=-1)\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrograms of heartbeat audio\n",
    "Spectral engineering is one of the most common techniques in machine learning for time series data. The first step in this process is to calculate a spectrogram of sound. This describes what spectral content (e.g., low and high pitches) are present in the sound over time.   \n",
    "In this exercise, you'll calculate a spectrogram of a heartbeat audio file.   \n",
    "We've loaded a single heartbeat sound in the variable audio.\n",
    "\n",
    "### Instructions\n",
    "Import the short-time fourier transform (stft) function from librosa.core.    \n",
    "Calculate the spectral content (using the short-time fourier transform function) of audio.  \n",
    "Convert the spectogram (spec) to decibels.   \n",
    "Visualize the spectogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stft function\n",
    "from librosa.core import stft\n",
    "\n",
    "# Prepare the STFT\n",
    "HOP_LENGTH = 2**4\n",
    "WINDOW_SIZE = 2**7\n",
    "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=WINDOW_SIZE)\n",
    "\n",
    "from librosa.core import amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Convert into decibels\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Compare the raw audio to the spectrogram of the audio\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "axs[0].plot(time, audio)\n",
    "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering spectral features\n",
    "As you can probably tell, there is a lot more information in a spectrogram compared to a raw audio file. By computing the spectral features, you have a much better idea of what's going on.    \n",
    "As such, there are all kinds of spectral features that you can compute using the spectrogram as a base. In this exercise, you'll look at a few of these features.     \n",
    "The spectogram spec from the previous exercise is available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "Calculate the spectral bandwidth as well as the spectral centroid of the spectrogram by using functions in librosa.feature.   \n",
    "Convert the spectrogram to decibels for visualization.    \n",
    "Plot the spectrogram over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lr\n",
    "\n",
    "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
    "\n",
    "from librosa.core import amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Convert spectrogram to decibels for visualization\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Display these features on top of the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
    "ax.plot(times_spec, centroids)\n",
    "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
    "ax.set(ylim=[None, 6000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining many features in a classifier\n",
    "You've spent this lesson engineering many features from the audio data - some contain information about how the audio changes in time, others contain information about the spectral content that is present.   \n",
    "The beauty of machine learning is that it can handle all of these features at the same time. If there is different information present in each feature, it should improve the classifier's ability to distinguish the types of audio. Note that this often requires more advanced techniques such as regularization, which we'll cover in the next chapter.    \n",
    "For the final exercise in the chapter, we've loaded many of the features that you calculated before. Combine all of them into an array that can be fed into the classifier, and see how it does.\n",
    "\n",
    "### Instructions \n",
    "Loop through each spectrogram, calculating the mean spectral bandwidth and centroid of each.    \n",
    "Column stack all the features to create the array X.    \n",
    "Score the classifier with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each spectrogram\n",
    "bandwidths = []\n",
    "centroids = []\n",
    "\n",
    "for spec in spectrograms:\n",
    "    # Calculate the mean spectral bandwidth\n",
    "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
    "    # Calculate the mean spectral centroid\n",
    "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
    "    # Collect the values\n",
    "    bandwidths.append(this_mean_bandwidth)  \n",
    "    centroids.append(this_mean_centroid)\n",
    "\n",
    "# Create X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
