{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a competition metric\n",
    "Competition metric is used by Kaggle to evaluate your submissions. Moreover, you also need to measure the performance of different models on a local validation set.\n",
    "\n",
    "For now, your goal is to manually develop a couple of competition metrics in case if they are not available in sklearn.metrics.\n",
    "\n",
    "In particular, you will define:\n",
    "\n",
    "### Mean Squared Error (MSE) for the regression problem:\n",
    "MSE=(1/N)∑(yi−ŷi)^2   \n",
    "\n",
    "Logarithmic Loss (LogLoss) for the binary classification problem:   \n",
    "\n",
    "LogLoss=(−1/N)∑(yilnpi+(1−yi)ln(1−pi))\n",
    "### Instructions\n",
    "Using numpy, define MSE metric. As a function input, you're given true y_true and predicted y_pred arrays.  \n",
    "Using numpy, define LogLoss metric. As input, you're given true class y_true and probability predicted prob_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import MSE from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define your own MSE function\n",
    "def own_mse(y_true, y_pred):\n",
    "  \t# Raise differences to the power of 2\n",
    "    squares = np.power(y_true - y_pred, 2)\n",
    "    # Find mean over all observations\n",
    "    err = np.mean(squares)\n",
    "    return err\n",
    "\n",
    "print('Sklearn MSE: {:.5f}. '.format(mean_squared_error(y_regression_true, y_regression_pred)))\n",
    "print('Your MSE: {:.5f}. '.format(own_mse(y_regression_true, y_regression_pred)))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import log_loss from sklearn\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Define your own LogLoss function\n",
    "def own_logloss(y_true, prob_pred):\n",
    "  \t# Find loss for each observation\n",
    "    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)\n",
    "    # Find mean over all observations\n",
    "    err = np.mean(terms) \n",
    "    return -err\n",
    "\n",
    "print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))\n",
    "print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA statistics\n",
    "As mentioned in the slides, you'll work with New York City taxi fare prediction data. You'll start with finding some basic statistics about the data. Then you'll move forward to plot some dependencies and generate hypotheses on them.\n",
    "\n",
    "The train and test DataFrames are already available in your workspace.\n",
    "\n",
    "### Instructions \n",
    "Find the shapes of the train and test data.   \n",
    "Look at the head of the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes of train and test data\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "\n",
    "# Train head()\n",
    "print(train.head())\n",
    "\n",
    "# Describe the target variable\n",
    "print(train.fare_amount.describe())\n",
    "\n",
    "# Train distribution of passengers within rides\n",
    "print(train.passenger_count.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA plots I\n",
    "After generating a couple of basic statistics, it's time to come up with and validate some ideas about the data dependencies. Again, the train DataFrame from the taxi competition is already available in your workspace.\n",
    "\n",
    "To begin with, let's make a scatterplot plotting the relationship between the fare amount and the distance of the ride. Intuitively, the longer the ride, the higher its price.\n",
    "\n",
    "To get the distance in kilometers between two geo-coordinates, you will use Haversine distance. Its calculation is available with the haversine_distance() function defined for you. The function expects train DataFrame as input.\n",
    "\n",
    "### Instructions\n",
    "Create a new variable \"distance_km\" as Haversine distance between pickup and dropoff points.  \n",
    "Plot a scatterplot with \"fare_amount\" on the x axis and \"distance_km\" on the y axis. To draw a scatterplot use matplotlib scatter() method.  \n",
    "Set a limit on a ride distance to be between 0 and 50 kilometers to avoid plotting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ride distance\n",
    "train['distance_km'] = haversine_distance(train)\n",
    "plt.clf()\n",
    "# Draw a scatterplot\n",
    "plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)\n",
    "plt.xlabel('Fare amount')\n",
    "plt.ylabel('Distance, km')\n",
    "plt.title('Fare amount based on the distance')\n",
    "\n",
    "# Limit on the distance\n",
    "plt.ylim(0, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA plots II\n",
    "Another idea that comes to mind is that the price of a ride could change during the day.\n",
    "\n",
    "Your goal is to plot the median fare amount for each hour of the day as a simple line plot. The hour feature is calculated for you. Don't worry if you do not know how to work with the date features. We will explore them in the chapter on Feature Engineering.\n",
    "\n",
    "### Instructions\n",
    "Group train DataFrame by \"hour\" and calculate the median for the \"fare_amount\" column.  \n",
    "Using hour_price DataFrame obtained, plot a line with \"hour\" on the x axis and \"fare_amount\" on the y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hour feature\n",
    "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
    "train['hour'] = train.pickup_datetime.dt.hour\n",
    "\n",
    "# Find median fare_amount for each hour\n",
    "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
    "\n",
    "# Plot the line plot\n",
    "plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')\n",
    "plt.xlabel('Hour of the day')\n",
    "plt.ylabel('Median fare amount')\n",
    "plt.title('Fare amount based on day time')\n",
    "plt.xticks(range(24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "You will start by getting hands-on experience in the most commonly used K-fold cross-validation.\n",
    "\n",
    "The data you'll be working with is from the \"Two sigma connect: rental listing inquiries\" Kaggle competition. The competition problem is a multi-class classification of the rental listings into 3 classes: low interest, medium interest and high interest. For faster performance, you will work with a subsample consisting of 1,000 observations.\n",
    "\n",
    "You need to implement a K-fold validation strategy and look at the sizes of each fold obtained. train DataFrame is already available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "Create a KFold object with 3 folds.  \n",
    "Loop over each split using the kf object.  \n",
    "For each split select training and testing folds using train_index and test_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "# Loop through each split\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Obtain training and testing folds\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    print('Fold: {}'.format(fold))\n",
    "    print('CV train shape: {}'.format(cv_train.shape))\n",
    "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
    "    fold += 1\n",
    "\n",
    "\"\"\" we see that the number of observations in each fold is almost uniform. \n",
    "It means that we've just splitted the train data into 3 equal folds. \n",
    "However, if we look at the number of medium-interest listings, \n",
    "it's varying from 162 to 175 from one fold to another. \n",
    "To make them uniform among the folds, let's use Stratified K-fold!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-fold\n",
    "As you've just noticed, you have a pretty different target variable distribution among the folds due to the random splits. It's not crucial for this particular competition, but could be an issue for the classification competitions with the highly imbalanced target variable.\n",
    "\n",
    "To overcome this, let's implement the stratified K-fold strategy with the stratification on the target variable. train DataFrame is already available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "Create a StratifiedKFold object with 3 folds and shuffling.  \n",
    "Loop over each split using str_kf object. Stratification is based on the \"interest_level\" column.  \n",
    "For each split select training and testing folds using train_index and test_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a StratifiedKFold object\n",
    "str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "# Loop through each split\n",
    "fold = 0\n",
    "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
    "    # Obtain training and testing folds\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    print('Fold: {}'.format(fold))\n",
    "    print('CV train shape: {}'.format(cv_train.shape))\n",
    "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
    "    fold += 1\n",
    "    \n",
    "\"\"\"  Now you see that both size and target distribution are the same among the folds. \n",
    "The general rule is to prefer Stratified K-Fold over usual K-Fold in any classification problem. \n",
    "Move to the next lesson to learn about other cross-validation strategies!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time K-fold\n",
    "Remember the \"Store Item Demand Forecasting Challenge\" where you are given store-item sales data, and have to predict future sales?\n",
    "\n",
    "It's a competition with time series data. So, time K-fold cross-validation should be applied. Your goal is to create this cross-validation strategy and make sure that it works as expected.\n",
    "\n",
    "Note that the train DataFrame is already available in your workspace, and that TimeSeriesSplit has been imported from sklearn.model_selection.\n",
    "\n",
    "### Instructions\n",
    "Create a TimeSeriesSplit object with 3 splits.  \n",
    "Sort the train data by \"date\" column to apply time K-fold.  \n",
    "Loop over each time split using time_kfold object.  \n",
    "For each split select training and testing folds using train_index and test_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TimeSeriesSplit object\n",
    "time_kfold = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Sort train data by date\n",
    "train = train.sort_values('date')\n",
    "\n",
    "# Iterate through each split\n",
    "fold = 0\n",
    "for train_index, test_index in time_kfold.split(train):\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "    print('Fold :', fold)\n",
    "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
    "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall validation score\n",
    "Now it's time to get the actual model performance using cross-validation! How does our store item demand prediction model perform?\n",
    "\n",
    "Your task is to take the Mean Squared Error (MSE) for each fold separately, and then combine these results into a single number.\n",
    "\n",
    "For simplicity, you're given get_fold_mse() function that for each cross-validation split fits a Random Forest model and returns a list of MSE scores by fold. get_fold_mse() accepts two arguments: train and TimeSeriesSplit object.\n",
    "\n",
    "### Instructions \n",
    "Create time 3-fold cross-validation.  \n",
    "Print the numpy mean of MSE scores by folds.  \n",
    "Print the list of MSEs by fold.   \n",
    "Calculate the overall score, find the sum of MSE mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Sort train data by date\n",
    "train = train.sort_values('date')\n",
    "\n",
    "# Initialize 3-fold time cross-validation\n",
    "kf = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Get MSE scores for each cross-validation split\n",
    "mse_scores = get_fold_mse(train, kf)\n",
    "\n",
    "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))\n",
    "print('MSE by fold: {}'.format(mse_scores))\n",
    "print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))\n",
    "\n",
    "\"\"\" Now, you know different validation strategies as well as how to use them to obtain \n",
    "overall model performance. It's time for the next and the most interesting part of the \n",
    "solution process: Feature Engineering and Modeling. See you in the next Chapters!\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
