{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character count of Russian tweets\n",
    "In this exercise, you have been given a dataframe tweets which contains some tweets associated with Russia's Internet Research Agency and compiled by FiveThirtyEight.   \n",
    "Your task is to create a new feature 'char_count' in tweets which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the content feature of tweets. \n",
    "Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).\n",
    "### Instructions\n",
    "Create a new feature char_count by applying len to the 'content' feature of tweets.  \n",
    "Print the average character count of the tweets by computing the mean of the 'char_count' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count of TED talks\n",
    "ted is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature word_count which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the transcript feature in ted.   \n",
    "In order to complete this task, you will need to define a function count_words that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the transcript feature of ted to create the new feature word_count and compute its mean.\n",
    "### Instructions\n",
    "Split string into a list of words using the split() method.   \n",
    "Return the number of elements in words using len().   \n",
    "Apply your function to the transcript column of ted to create the new feature word_count.  \n",
    "Compute the average word count of the talks using mean()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags and mentions in Russian tweets\n",
    "Let's revisit the tweets dataframe containing the Russian tweets. In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions count_hashtags() and count_mentions() respectively and applying them to the content feature of tweets.   \n",
    "In case you don't recall, the tweets are contained in the content feature of tweets.    \n",
    "### Instructions\n",
    "In the list comprehension, use startswith() to check if a particular word starts with '#'.   \n",
    "In the list comprehension, use startswith() to check if a particular word starts with '@'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability of 'The Myth of Sisyphus'\n",
    "In this exercise, you will compute the Flesch reading ease score for Albert Camus' famous essay The Myth of Sisyphus. We will then interpret the value of this score as explained in the video and try to determine the reading level of the essay.   \n",
    "The entire essay is in the form of a string and is available as sisyphus_essay.\n",
    "### Instructions\n",
    "Import the Textatistic class from textatistic.    \n",
    "Compute the readability_scores dictionary for sisyphus_essay using Textatistic.  \n",
    "Print the Flesch reading ease score from the readability_scores dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n",
    "\n",
    "\"\"\"\n",
    "In [3]: type(readability_scores)\n",
    "Out[3]: dict\n",
    "\n",
    "In [4]: readability_scores\n",
    "Out[4]: \n",
    "{'dalechall_score': 7.487433762517882,\n",
    " 'flesch_score': 81.67466335836913,\n",
    " 'fleschkincaid_score': 5.485083154506439,\n",
    " 'gunningfog_score': 7.913698140200286,\n",
    " 'smog_score': 8.110721755262034}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability of various publications\n",
    "In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog index and consequently, determine the relative difficulty of reading these publications.   \n",
    "The excerpts are available as the following strings:   \n",
    "forbes- An excerpt from an article from Forbes magazine on the Chinese social credit score system.   \n",
    "harvard_law- An excerpt from a book review published in Harvard Law Review.  \n",
    "r_digest- An excerpt from a Reader's Digest article on flight turbulence.  \n",
    "time_kids - An excerpt from an article on the ill effects of salt consumption published in TIME for Kids.\n",
    "### Instructions\n",
    "Import the Textatistic class from textatistic.   \n",
    "Compute the readability_scores dictionary for each excerpt using Textatistic.  \n",
    "Select the Gunning fog index from the readability_scores dictionary for each excerpt and append it to gunning_fog_scores.  \n",
    "Print the list of Gunning fog indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
