{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the digits\n",
    "You're going to build a model on the digits dataset, a sample dataset that comes pre-loaded with scikit learn. The digits dataset consist of 8x8 pixel handwritten digits from 0 to 9:   \n",
    "\n",
    "You want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification.   \n",
    "The dataset has already been partitioned into X_train, y_train, X_test, and y_test, using 30% of the data as testing data. The labels are already one-hot encoded vectors, so you don't need to use Keras to_categorical() function.   \n",
    "\n",
    "Let's build this new model!\n",
    "\n",
    "### Instructions\n",
    "Add a Dense layer of 16 neurons with relu activation and an input_shape that takes the total number of pixels of the 8x8 digit image.  \n",
    "Add a Dense layer with 10 outputs and softmax activation.  \n",
    "Compile your model with adam, categorical_crossentropy, and accuracy metrics.  \n",
    "Make sure your model works by predicting on X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape=(64,), activation='relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Test if your model is well assembled by predicting before training\n",
    "print(model.predict(X_train))\n",
    "\n",
    "\"\"\" Predicting on training data inputs before training can help you quickly check that \n",
    "your model works as expected.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model overfitting?\n",
    "Let's train the model you just built and plot its learning curve to check out if it's overfitting! You can make use of the loaded function plot_loss() to plot training loss against validation loss, you can get both from the history callback.\n",
    "\n",
    "If you want to inspect the plot_loss() function code, paste this in the console: show_code(plot_loss)\n",
    "\n",
    "### Instructions\n",
    "Train your model for 60 epochs, using X_test and y_test as validation data.\n",
    "Use plot_loss() passing loss and val_loss as extracted from the history attribute of the h_callback object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "h_callback = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the h_callback object loss and val_loss to plot the learning curve\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we need more data?\n",
    "It's time to check whether the digits dataset model you built benefits from more training examples!\n",
    "\n",
    "In order to keep code to a minimum, various things are already initialized and ready to use:   \n",
    "\n",
    "The model you just built.  \n",
    "X_train,y_train,X_test, and y_test.  \n",
    "The initial_weights of your model, saved after using model.get_weights().  \n",
    "A pre-defined list of training sizes: training_sizes.  \n",
    "A pre-defined early stopping callback monitoring loss: early_stop.  \n",
    "Two empty lists to store the evaluation results: train_accs and test_accs.  \n",
    "Train your model on the different training sizes and evaluate the results on X_test. End by plotting the results with plot_results().  \n",
    "\n",
    "The full code for this exercise can be found on the slides!  \n",
    "\n",
    "### Instructions\n",
    "Get a fraction of the training data determined by the size we are currently evaluating in the loop.  \n",
    "Set the model weights to the initial_weights with set_weights() and train your model on the fraction of training data using early_stop as a callback.   \n",
    "Evaluate and store the accuracy for the training fraction and the test set.  \n",
    "Call plot_results() passing in the training and test accuracies for each training size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in training_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "\n",
    "    # Reset the model to the initial weights and train it on the new training data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store both: the training data fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing activation functions\n",
    "Comparing activation functions involves a bit of coding, but nothing you can't do! \n",
    "\n",
    "You will try out different activation functions on the multi-label model you built for your farm irrigation machine in chapter 2. The function get_model('relu') returns a copy of this model and applies the 'relu' activation function to its hidden layer. \n",
    "\n",
    "You will loop through several activation functions, generate a new model for each and train it. By storing the history callback in a dictionary you will be able to visualize which activation function performed best in the next exercise!\n",
    "\n",
    "X_train, y_train, X_test, y_test are ready for you to use when training your models.   \n",
    "\n",
    "### Instructions\n",
    "Fill up the activation functions array with relu,leaky_relu, sigmoid, and tanh.   \n",
    "Get a new model for each iteration with get_model() passing the current activation function as a parameter.  \n",
    "Fit your model providing the train and validation_data, use 20 epochs and set verbose to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=0)\n",
    "  activation_results[act] = h_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing activation functions II\n",
    "What you coded in the previous exercise has been executed to obtain the activation_results variable, this time 100 epochs were used instead of 20. This way you will have more epochs to further compare how the training evolves per activation function.   \n",
    "\n",
    "For every h_callback of each activation function in activation_results:  \n",
    " \n",
    "The h_callback.history['val_loss'] has been extracted.  \n",
    "The h_callback.history['val_acc'] has been extracted.   \n",
    "Both are saved into two dictionaries: val_loss_per_function and val_acc_per_function.  \n",
    "\n",
    "Pandas is also loaded as pd for you to use. Let's plot some quick validation loss and accuracy charts!  \n",
    "\n",
    "### Instructions\n",
    "Use pd.DataFrame()to create a new DataFrame from the val_loss_per_function dictionary.   \n",
    "Call plot() on the DataFrame.  \n",
    "Create another pandas DataFrame from val_acc_per_function.  \n",
    "Once again, plot the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss = pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a model for tuning\n",
    "Let's tune the hyperparameters of a binary classification model that does well classifying the breast cancer dataset.   \n",
    "\n",
    "You've seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. The definition of this function is important since hyperparameter tuning is carried out by varying the arguments your function receives.  \n",
    "  \n",
    "Build a simple create_model() function that receives both a learning rate and an activation function as arguments. The Adam optimizer has been imported as an object from keras.optimizers so that you can also change its learning rate parameter.  \n",
    "\n",
    "### Instructions\n",
    "Set the learning rate of the Adam optimizer object to the one passed in the arguments.\n",
    "Set the hidden layers activations to the one passed in the arguments.\n",
    "Pass the optimizer and the binary cross-entropy loss to the .compile() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "  \n",
    "  \t# Create an Adam optimizer with the given learning rate\n",
    "  \topt = Adam(lr=learning_rate)\n",
    "  \t\n",
    "  \t# Create your binary classification model  \n",
    "  \tmodel = Sequential()\n",
    "  \tmodel.add(Dense(128, input_shape=(30,), activation=activation))\n",
    "  \tmodel.add(Dense(256, activation=activation))\n",
    "  \tmodel.add(Dense(1, activation='sigmoid'))\n",
    "  \t\n",
    "  \t# Compile your model with your optimizer, loss, and metrics\n",
    "  \tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  \treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the model parameters\n",
    "It's time to try out different parameters on your model and see how well it performs!   \n",
    "\n",
    "The create_model() function you built in the previous exercise is ready for you to use.  \n",
    "\n",
    "Since fitting the RandomizedSearchCV object would take too long, the results you'd get are printed in the show_results() function. You could try random_search.fit(X,y) in the console yourself to check it does work after you have built everything else, but you will probably timeout the exercise (so copy your code first if you try this or you can lose your progress!).   \n",
    "\n",
    "You don't need to use the optional epochs and batch_size parameters when building your KerasClassifier object since you are passing them as params to the random search and this works already.   \n",
    "\n",
    "### Instructions\n",
    "Import KerasClassifier from keras scikit_learn wrappers.   \n",
    "Use your create_model function when instantiating your KerasClassifier.  \n",
    "Set 'relu' and 'tanh' as activation, 32, 128, and 256 as batch_size, 50, 100, and 200 epochs, and learning_rate of 0.1, 0.01, and 0.001.   \n",
    "Pass your converted model and the chosen params as you build your RandomizedSearchCV object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras scikit learn wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
    "          'epochs': [50, 100, 200], 'learning_rate': [.1, .01, .001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions=params, cv=KFold(3))\n",
    "\n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()\n",
    "\n",
    "\"\"\" <script.py> output:\n",
    "    Best: \n",
    "    0.975395 using {learning_rate: 0.001, epochs: 50, batch_size: 128, activation: relu} \n",
    "    Other: \n",
    "    0.956063 (0.013236) with: {learning_rate: 0.1, epochs: 200, batch_size: 32, activation: tanh} \n",
    "    0.970123 (0.019838) with: {learning_rate: 0.1, epochs: 50, batch_size: 256, activation: tanh} \n",
    "    0.971880 (0.006524) with: {learning_rate: 0.01, epochs: 100, batch_size: 128, activation: tanh} \n",
    "    0.724077 (0.072993) with: {learning_rate: 0.1, epochs: 50, batch_size: 32, activation: relu} \n",
    "    0.588752 (0.281793) with: {learning_rate: 0.1, epochs: 100, batch_size: 256, activation: relu} \n",
    "    0.966608 (0.004892) with: {learning_rate: 0.001, epochs: 100, batch_size: 128, activation: tanh} \n",
    "    0.952548 (0.019734) with: {learning_rate: 0.1, epochs: 50, batch_size: 256, activation: relu} \n",
    "    0.971880 (0.006524) with: {learning_rate: 0.001, epochs: 200, batch_size: 128, activation: relu}\n",
    "    0.968366 (0.004239) with: {learning_rate: 0.01, epochs: 100, batch_size: 32, activation: relu}\n",
    "    0.910369 (0.055824) with: {learning_rate: 0.1, epochs: 100, batch_size: 128, activation: relu}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with cross-validation\n",
    "Time to train your model with the best parameters found: 0.001 for the learning rate, 50 epochs, a 128 batch_size and relu activations.   \n",
    "\n",
    "The create_model() function from the previous exercise is ready for you to use. X and y are loaded as features and labels.  \n",
    "\n",
    "Use the best values found for your model when creating your KerasClassifier object so that they are used when performing cross_validation.   \n",
    "\n",
    "End this chapter by training an awesome tuned model on the breast cancer dataset!\n",
    "\n",
    "### Instructions\n",
    "Import KerasClassifier from keras scikit_learn wrappers.  \n",
    "Create a KerasClassifier object providing the best parameters found.  \n",
    "Pass your model, features and labels to cross_val_score to perform cross-validation with 3 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model(learning_rate=.001, activation='relu'), \n",
    "                        epochs=50, \n",
    "                        batch_size=128,\n",
    "                        verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv=3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())\n",
    "\n",
    "\"\"\" <script.py> output:\n",
    "    The mean accuracy was: 0.9718834066666666\n",
    "    With a standard deviation of: 0.002448915612216046\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
