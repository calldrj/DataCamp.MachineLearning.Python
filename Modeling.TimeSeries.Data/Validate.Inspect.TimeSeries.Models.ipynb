{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating time-shifted features\n",
    "In machine learning for time series, it's common to use information about previous time points to predict a subsequent time point.    \n",
    "In this exercise, you'll \"shift\" your raw data and visualize the results. You'll use the percent change time series that you calculated in the previous chapter, this time with a very short window. A short window is important because, in a real-world scenario, you want to predict the day-to-day fluctuations of a time series, not its change over a longer window of time.   \n",
    "\n",
    "### Instructions\n",
    "Use a dictionary comprehension to create multiple time-shifted versions of prices_perc using the lags specified in shifts.     \n",
    "Convert the result into a DataFrame.   \n",
    "Use the given code to visualize the results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the \"time lags\"\n",
    "shifts = np.arange(1, 11).astype(int)\n",
    "\n",
    "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
    "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
    "\n",
    "# Convert into a DataFrame for subsequent use\n",
    "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
    "\n",
    "# Plot the first 100 samples of each\n",
    "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
    "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case: Auto-regressive models\n",
    "Now that you've created time-shifted versions of a single time series, you can fit an auto-regressive model. This is a regression model where the input features are time-shifted versions of the output time series data. You are using previous values of a timeseries to predict current values of the same timeseries (thus, it is auto-regressive).   \n",
    "By investigating the coefficients of this model, you can explore any repetitive patterns that exist in a timeseries, and get an idea for how far in the past a data point is predictive of the future.\n",
    "### Instructions\n",
    "Replace missing values in prices_perc_shifted with the median of the DataFrame and assign it to X.  \n",
    "Replace missing values in prices_perc with the median of the series and assign it to y.   \n",
    "Fit a regression model using the X and y arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with the median for each column\n",
    "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
    "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
    "\n",
    "# Fit the model\n",
    "model = Ridge()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize regression coefficients\n",
    "Now that you've fit the model, let's visualize its coefficients. This is an important part of machine learning because it gives you an idea for how the different features of a model affect the outcome.    \n",
    "The shifted time series DataFrame (prices_perc_shifted) and the regression model (model) are available in your workspace.    \n",
    "In this exercise, you will create a function that, given a set of coefficients and feature names, visualizes the coefficient values.  \n",
    "### Instructions\n",
    "Define a function (called visualize_coefficients) that takes as input an array of coefficients, an array of each coefficient's name, and an instance of a Matplotlib axis object. It should then generate a bar plot for the input coefficients, with their names on the x-axis.   \n",
    "Use this function (visualize_coefficients()) with the coefficients contained in the model variable and column names of prices_perc_shifted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefficients(coefs, names, ax):\n",
    "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
    "    ax.bar(names, coefs)\n",
    "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
    "    \n",
    "    # Set formatting so it looks nice\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    return ax\n",
    "\n",
    "# Visualize the output data up to \"2011-01\"\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "y.loc[:'2011-01'].plot(ax=axs[0])\n",
    "\n",
    "# Run the function to visualize model's coefficients\n",
    "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-regression with a smoother time series\n",
    "Now, let's re-run the same procedure using a smoother signal. You'll use the same percent change algorithm as before, but this time use a much larger window (40 instead of 20). As the window grows, the difference between neighboring timepoints gets smaller, resulting in a smoother signal. What do you think this will do to the auto-regressive model?   \n",
    "prices_perc_shifted and model (updated to use a window of 40) are available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "Using the function (visualize_coefficients()) you created in the last exercise, generate a plot with coefficients of model and column names of prices_perc_shifted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the output data up to \"2011-01\"\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "y.loc[:'2011-01'].plot(ax=axs[0])\n",
    "\n",
    "# Run the function to visualize model's coefficients\n",
    "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
    "plt.show()\n",
    "\n",
    "\"\"\" As you can see here, by transforming your data with a larger window, \n",
    "you've also changed the relationship between each timepoint and the ones \n",
    "that come just before it. This model's coefficients gradually go down to zero, \n",
    "which means that the signal itself is smoother over time. Be careful \n",
    "when you see something like this, as it means your data is not i.i.d.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation with shuffling\n",
    "As you'll recall, cross-validation is the process of splitting your data into training and test sets multiple times. Each time you do this, you choose a different training and test set. In this exercise, you'll perform a traditional ShuffleSplit cross-validation on the company value data from earlier. Later we'll cover what changes need to be made for time series data. The data we'll use is the same historical price data for several large companies.    \n",
    "An instance of the Linear regression object (model) is available in your workspace along with the function r2_score() for scoring. Also, the data is stored in arrays X and y. We've also provided a helper function (visualize_predictions()) to help visualize the results.\n",
    "### Instructions\n",
    "Initialize a ShuffleSplit cross-validation object with 10 splits.   \n",
    "Iterate through CV splits using this object. On each iteration:  \n",
    "Fit a model using the training indices.   \n",
    "Generate predictions using the test indices, score the model (R2) using the predictions, and collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ShuffleSplit and create the cross-validation object\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, random_state=1)\n",
    "\n",
    "# Iterate through CV splits\n",
    "results = []\n",
    "for tr, tt in cv.split(X, y):\n",
    "    # Fit the model on training data\n",
    "    model.fit(X[tr], y[tr])\n",
    "    \n",
    "    # Generate predictions on the test data, score the predictions, and collect\n",
    "    prediction = model.predict(X[tt])\n",
    "    score = r2_score(y[tt], prediction)\n",
    "    results.append((prediction, score, tt))\n",
    "\n",
    "# Custom function to quickly visualize predictions\n",
    "visualize_predictions(results)\n",
    "\n",
    "\"\"\"  If you look at the plot to the right, see that the order of datapoints in \n",
    "the test set is scrambled. Let's see how it looks when we shuffle the data in blocks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation without shuffling\n",
    "Now, re-run your model fit using block cross-validation (without shuffling all datapoints). In this case, neighboring time-points will be kept close to one another. How do you think the model predictions will look in each cross-validation loop?\n",
    "\n",
    "An instance of the Linear regression model object is available in your workspace. Also, the arrays X and y (training data) are available too.\n",
    "\n",
    "### Instructions\n",
    "Instantiate another cross-validation object, this time using KFold cross-validation with 10 splits and no shuffling.   \n",
    "Iterate through this object to fit a model using the training indices and generate predictions using the test indices.   \n",
    "Visualize the predictions across CV splits using the helper function (visualize_predictions()) we've provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KFold cross-validation object\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
    "\n",
    "# Iterate through CV splits\n",
    "results = []\n",
    "for tr, tt in cv.split(X, y):\n",
    "    # Fit the model on training data\n",
    "    model.fit(X[tr], y[tr])\n",
    "    \n",
    "    # Generate predictions on the test data and collect\n",
    "    prediction = model.predict(X[tt])\n",
    "    results.append((prediction, tt))\n",
    "    \n",
    "# Custom function to quickly visualize predictions\n",
    "visualize_predictions(results)\n",
    "\n",
    "\"\"\" This time, the predictions generated within each CV loop\n",
    "looks 'smoother' than they were before - they look more like \n",
    "a real time series because you didn't shuffle the data. \n",
    "This is a good sanity check to make sure your CV splits are correct.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-based cross-validation\n",
    "Finally, let's visualize the behavior of the time series cross-validation iterator in scikit-learn. Use this object to iterate through your data one last time, visualizing the training data used to fit the model on each iteration.  \n",
    "An instance of the Linear regression model object is available in your workspace. Also, the arrays X and y (training data) are available too.\n",
    "### Instructions\n",
    "Import TimeSeriesSplit from sklearn.model_selection.    \n",
    "Instantiate a time series cross-validation iterator with 10 splits.   \n",
    "Iterate through CV splits. On each iteration, visualize the values of the input data that would be used to train the model for that iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create time-series cross-validation object\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# Iterate through CV splits\n",
    "fig, ax = plt.subplots()\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Plot the training data on each iteration, to see the behavior of the CV\n",
    "    ax.plot(tr, ii + y[tr])\n",
    "\n",
    "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Note that the size of the training set grew each time when you used \n",
    "the time series cross-validation object. This way, the time points you \n",
    "predict are always after the timepoints we train on.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping a confidence interval\n",
    "A useful tool for assessing the variability of some data is the bootstrap. In this exercise, you'll write your own bootstrapping function that can be used to return a bootstrapped confidence interval.   \n",
    "This function takes three parameters: a 2-D array of numbers (data), a list of percentiles to calculate (percentiles), and the number of boostrap iterations to use (n_boots). It uses the resample function to generate a bootstrap sample, and then repeats this many times to calculate the confidence interval.   \n",
    "\n",
    "### Instructions\n",
    "The function should loop over the number of bootstraps (given by the parameter n_boots) and:  \n",
    "Take a random sample of the data, with replacement, and calculate the mean of this random sample\n",
    "Compute the percentiles of bootstrap_means and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
    "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
    "    # Create our empty array to fill the results\n",
    "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
    "    for ii in range(n_boots):\n",
    "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
    "        random_sample = resample(data)\n",
    "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
    "        \n",
    "    # Compute the percentiles of choice for the bootstrapped means\n",
    "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
    "    return percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating variability in model coefficients\n",
    "In this lesson, you'll re-run the cross-validation routine used before, but this time paying attention to the model's stability over time. You'll investigate the coefficients of the model, as well as the uncertainty in its predictions.   \n",
    "Begin by assessing the stability (or uncertainty) of a model's coefficients across multiple CV splits. Remember, the coefficients are a reflection of the pattern that your model has found in the data.   \n",
    "An instance of the Linear regression object (model) is available in your workpsace. Also, the arrays X and y (the data) are available too.\n",
    "\n",
    "### Instructions\n",
    "Initialize a TimeSeriesSplit cross-validation object   \n",
    "Create an array of all zeros to collect the coefficients.   \n",
    "Iterate through splits of the cross-validation object. On each iteration:   \n",
    "Fit the model on training data  \n",
    "Collect the model's coefficients for analysis later    \n",
    "Finally, calculate the 95% confidence interval for each coefficient in coefficients using the bootstrap_interval() function you defined in the previous exercise.    \n",
    "You can run bootstrap_interval? if you want a refresher on the parameters that this function takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through CV splits\n",
    "n_splits = 100\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Create empty array to collect coefficients\n",
    "coefficients = np.zeros([n_splits, X.shape[1]])\n",
    "\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Fit the model on training data and collect the coefficients\n",
    "    model.fit(X[tr], y[tr])\n",
    "    coefficients[ii] = model.coef_\n",
    "\n",
    "# Calculate a confidence interval around each coefficient\n",
    "bootstrapped_interval = bootstrap_interval(coefficients, (2.5, 97.5), 100)\n",
    "\n",
    "# Plot it\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
    "ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
    "ax.set(title='95% confidence interval for model coefficients')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing model score variability over time\n",
    "Now that you've assessed the variability of each coefficient, let's do the same for the performance (scores) of the model. Recall that the TimeSeriesSplit object will use successively-later indices for each test set. This means that you can treat the scores of your validation as a time series. You can visualize this over time in order to see how the model's performance changes over time.   \n",
    "An instance of the Linear regression model object is stored in model, a cross-validation object in cv, and data in X and y.\n",
    "\n",
    "### Instructions  \n",
    "Calculate the cross-validated scores of the model on the data (using a custom scorer we defined for you, my_pearsonr along with cross_val_score).   \n",
    "Convert the output scores into a pandas Series so that you can treat it as a time series.  \n",
    "Bootstrap a rolling confidence interval for the mean score using bootstrap_interval()  \n",
    "Run the given code to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Generate scores for each split to see how the model performs over time\\\n",
    "# cv is a TimeSeriesSplit object, cv = TimeSeriesSplit(n_split=predefined number)\n",
    "scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "\n",
    "# Convert to a Pandas Series object\n",
    "# type(times_scores): pandas.core.indexes.datetimes.DatetimeIndex\n",
    "scores_series = pd.Series(scores, index=times_scores, name='score')\n",
    "\n",
    "# Bootstrap a rolling confidence interval for the mean score\n",
    "scores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5))\n",
    "scores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5))\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "scores_lo.plot(ax=ax, label=\"Lower confidence interval\")\n",
    "scores_hi.plot(ax=ax, label=\"Upper confidence interval\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" You plotted a rolling confidence interval for scores over time. \n",
    "This is useful in seeing when your model predictions are correct.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting for non-stationarity\n",
    "In this exercise, you will again visualize the variations in model scores, but now for data that changes its statistics over time.   \n",
    "An instance of the Linear regression model object is stored in model, a cross-validation object in cv, and the data in X and y.\n",
    "\n",
    "### Instructions \n",
    "Create an empty DataFrame to collect the results.   \n",
    "Iterate through multiple window sizes, each time creating a new TimeSeriesSplit object.   \n",
    "Calculate the cross-validated scores (using a custom scorer we defined for you, my_pearsonr) of the model on training data.   \n",
    "Run the given code to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-initialize window sizes\n",
    "window_sizes = [25, 50, 75, 100]\n",
    "\n",
    "# Create an empty DataFrame to collect the stores\n",
    "all_scores = pd.DataFrame(index=times_scores)\n",
    "\n",
    "# Generate scores for each split to see how the model performs over time\n",
    "for window in window_sizes:\n",
    "    # Create cross-validation object using a limited lookback window\n",
    "    cv = TimeSeriesSplit(n_splits=100, max_train_size=window)\n",
    "    \n",
    "    # Calculate scores across all CV splits and collect them in a DataFrame\n",
    "    this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "    all_scores['Length {}'.format(window)] = this_scores\n",
    "    \n",
    "# Visualize the scores\n",
    "ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm)\n",
    "ax.set(title='Scores for multiple windows', ylabel='Correlation (r)')\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Notice how in some stretches of time, longer windows perform worse than shorter ones. \n",
    "This is because the statistics in the data have changed, and the longer window is now \n",
    "using outdated information.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
